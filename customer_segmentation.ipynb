{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0f590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADVANCED CUSTOMER SEGMENTATION ANALYTICS SUITE - COLAB OPTIMIZED\n",
    "# Enterprise-Grade ML Pipeline for Strategic Customer Intelligence\n",
    "# ==============================================================================\n",
    "\n",
    "# STEP 1: Install required packages (run this first in Colab)\n",
    "\"\"\"\n",
    "!pip install umap-learn hdbscan sentence-transformers xgboost lightgbm plotly --quiet\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ ADVANCED CUSTOMER SEGMENTATION ANALYTICS SUITE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Core ML & Statistical Libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Advanced ML Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import networkx as nx\n",
    "\n",
    "# NLP & Deep Learning\n",
    "from sentence_transformers import SentenceTransformer\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LSTM, Embedding\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    from tensorflow.keras import backend as K\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TensorFlow not available - some features will be skipped\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Advanced Clustering & Visualization\n",
    "import hdbscan\n",
    "import umap\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Statistical Testing\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# CRITICAL FIX: Proper Colab configuration for Plotly\n",
    "import plotly.offline as pyo\n",
    "from IPython.display import display, HTML\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "# Enable interactive plots in Colab\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: ENHANCED DATA LOADING & EXPLORATION\n",
    "# ==============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Enhanced data loading with comprehensive profiling\"\"\"\n",
    "\n",
    "    def __init__(self, file_path='int_online_tx.csv'):\n",
    "        self.file_path = file_path\n",
    "        self.df_raw = None\n",
    "        self.data_profile = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.df_raw = pd.read_csv(self.file_path)\n",
    "            print(f\"‚úÖ Data loaded: {self.df_raw.shape[0]:,} rows √ó {self.df_raw.shape[1]} columns\")\n",
    "            self.profile_data()\n",
    "            return self.df_raw\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Error: '{self.file_path}' not found.\")\n",
    "            print(\"üìÅ Creating sample data for demonstration...\")\n",
    "            return self.create_sample_data()\n",
    "\n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Create sample e-commerce data for demonstration\"\"\"\n",
    "        np.random.seed(42)\n",
    "        n_customers = 500\n",
    "        n_transactions = 5000\n",
    "        \n",
    "        # Sample customer data\n",
    "        customers = np.random.randint(10000, 20000, n_customers)\n",
    "        \n",
    "        data = []\n",
    "        for _ in range(n_transactions):\n",
    "            customer_id = np.random.choice(customers)\n",
    "            invoice_date = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))\n",
    "            quantity = np.random.randint(1, 50)\n",
    "            unit_price = np.random.uniform(1.0, 100.0)\n",
    "            stock_code = f\"ITEM{np.random.randint(1000, 9999)}\"\n",
    "            description = f\"Product {np.random.choice(['Electronics', 'Clothing', 'Home', 'Books', 'Sports'])} Item\"\n",
    "            \n",
    "            data.append({\n",
    "                'InvoiceNo': f\"INV{np.random.randint(10000, 99999)}\",\n",
    "                'StockCode': stock_code,\n",
    "                'Description': description,\n",
    "                'Quantity': quantity,\n",
    "                'InvoiceDate': invoice_date,\n",
    "                'UnitPrice': unit_price,\n",
    "                'CustomerID': customer_id,\n",
    "                'Country': np.random.choice(['UK', 'USA', 'Germany', 'France'])\n",
    "            })\n",
    "        \n",
    "        self.df_raw = pd.DataFrame(data)\n",
    "        print(f\"‚úÖ Sample data created: {self.df_raw.shape[0]:,} rows √ó {self.df_raw.shape[1]} columns\")\n",
    "        self.profile_data()\n",
    "        return self.df_raw\n",
    "\n",
    "    def profile_data(self):\n",
    "        \"\"\"Comprehensive data profiling\"\"\"\n",
    "        self.data_profile = {\n",
    "            'shape': self.df_raw.shape,\n",
    "            'memory_usage': self.df_raw.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'missing_values': self.df_raw.isnull().sum(),\n",
    "            'duplicates': self.df_raw.duplicated().sum(),\n",
    "            'unique_customers': self.df_raw['CustomerID'].nunique() if 'CustomerID' in self.df_raw else 0,\n",
    "            'date_range': self.get_date_range(),\n",
    "            'numeric_summary': self.df_raw.select_dtypes(include=[np.number]).describe()\n",
    "        }\n",
    "\n",
    "        print(f\"üìä Data Profile:\")\n",
    "        print(f\"    Memory Usage: {self.data_profile['memory_usage']:.2f} MB\")\n",
    "        print(f\"    Unique Customers: {self.data_profile['unique_customers']:,}\")\n",
    "        print(f\"    Duplicates: {self.data_profile['duplicates']:,}\")\n",
    "\n",
    "    def get_date_range(self):\n",
    "        if 'InvoiceDate' in self.df_raw.columns:\n",
    "            dates = pd.to_datetime(self.df_raw['InvoiceDate'])\n",
    "            return {'start': dates.min(), 'end': dates.max(), 'days': (dates.max() - dates.min()).days}\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: ADVANCED FEATURE ENGINEERING FRAMEWORK\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedFeatureEngineer:\n",
    "    \"\"\"Comprehensive feature engineering with statistical validation\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.features = {}\n",
    "        self.feature_importance = {}\n",
    "\n",
    "    def create_temporal_features(self):\n",
    "        \"\"\"Advanced temporal pattern analysis\"\"\"\n",
    "        print(\"üïê Engineering temporal features...\")\n",
    "\n",
    "        # Convert InvoiceDate to datetime\n",
    "        self.df['InvoiceDate'] = pd.to_datetime(self.df['InvoiceDate'])\n",
    "        self.df['Sales'] = self.df['Quantity'] * self.df['UnitPrice']\n",
    "\n",
    "        # Reference date for recency calculations\n",
    "        reference_date = self.df['InvoiceDate'].max() + timedelta(days=1)\n",
    "\n",
    "        # Customer-level temporal aggregations\n",
    "        temporal_features = self.df.groupby('CustomerID').agg({\n",
    "            'InvoiceDate': [\n",
    "                ('recency_days', lambda x: (reference_date - x.max()).days),\n",
    "                ('first_purchase', 'min'),\n",
    "                ('last_purchase', 'max'),\n",
    "                ('purchase_span_days', lambda x: (x.max() - x.min()).days if len(x) > 1 else 0),\n",
    "                ('purchase_frequency', 'count')\n",
    "            ],\n",
    "            'InvoiceNo': [('total_orders', 'nunique')],\n",
    "            'Sales': [\n",
    "                ('total_revenue', 'sum'),\n",
    "                ('avg_order_value', 'mean'),\n",
    "                ('revenue_volatility', 'std'),\n",
    "                ('max_order_value', 'max'),\n",
    "                ('min_order_value', 'min')\n",
    "            ],\n",
    "            'Quantity': [\n",
    "                ('total_quantity', 'sum'),\n",
    "                ('avg_basket_size', 'mean'),\n",
    "                ('quantity_volatility', 'std')\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Flatten column names\n",
    "        temporal_features.columns = ['_'.join(col).strip() for col in temporal_features.columns.values]\n",
    "\n",
    "        # Advanced temporal metrics\n",
    "        if 'InvoiceDate_purchase_span_days' in temporal_features.columns:\n",
    "            temporal_features['purchase_regularity'] = (\n",
    "                temporal_features['InvoiceDate_purchase_span_days'] / temporal_features['InvoiceNo_total_orders']\n",
    "            ).fillna(0)\n",
    "        else:\n",
    "            temporal_features['purchase_regularity'] = 0\n",
    "\n",
    "        if 'InvoiceDate_last_purchase' in temporal_features.columns and 'InvoiceDate_first_purchase' in temporal_features.columns:\n",
    "            temporal_features['customer_lifetime_days'] = (\n",
    "                temporal_features['InvoiceDate_last_purchase'] - temporal_features['InvoiceDate_first_purchase']\n",
    "            ).dt.days\n",
    "        else:\n",
    "            temporal_features['customer_lifetime_days'] = 0\n",
    "\n",
    "        if 'Sales_total_revenue' in temporal_features.columns and 'customer_lifetime_days' in temporal_features.columns:\n",
    "            temporal_features['revenue_per_day'] = (\n",
    "                temporal_features['Sales_total_revenue'] / (temporal_features['customer_lifetime_days'] + 1)\n",
    "            )\n",
    "        else:\n",
    "            temporal_features['revenue_per_day'] = 0\n",
    "\n",
    "        # Seasonal patterns\n",
    "        seasonal_data = self.create_seasonal_features()\n",
    "        temporal_features = temporal_features.join(seasonal_data)\n",
    "\n",
    "        # Clean up\n",
    "        cols_to_drop = ['InvoiceDate_first_purchase', 'InvoiceDate_last_purchase']\n",
    "        temporal_features.drop(columns=cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "        temporal_features.fillna(0, inplace=True)\n",
    "\n",
    "        self.features['temporal'] = temporal_features\n",
    "        print(f\"    ‚úÖ Created {temporal_features.shape[1]} temporal features\")\n",
    "        return temporal_features\n",
    "\n",
    "    def create_seasonal_features(self):\n",
    "        \"\"\"Advanced seasonal and cyclical patterns\"\"\"\n",
    "        df_seasonal = self.df.copy()\n",
    "        df_seasonal['month'] = df_seasonal['InvoiceDate'].dt.month\n",
    "        df_seasonal['day_of_week'] = df_seasonal['InvoiceDate'].dt.dayofweek\n",
    "        df_seasonal['quarter'] = df_seasonal['InvoiceDate'].dt.quarter\n",
    "        df_seasonal['is_weekend'] = df_seasonal['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        seasonal_agg = df_seasonal.groupby('CustomerID').agg({\n",
    "            'month': [('favorite_month', lambda x: x.mode().iloc[0] if not x.mode().empty else 1)],\n",
    "            'day_of_week': [('favorite_day', lambda x: x.mode().iloc[0] if not x.mode().empty else 0)],\n",
    "            'is_weekend': [('weekend_shopper', 'mean')],\n",
    "            'quarter': [('seasonal_spread', 'nunique')]\n",
    "        })\n",
    "\n",
    "        seasonal_agg.columns = ['_'.join(col).strip() for col in seasonal_agg.columns.values]\n",
    "        return seasonal_agg\n",
    "\n",
    "    def create_product_diversity_features(self):\n",
    "        \"\"\"Advanced product portfolio analysis\"\"\"\n",
    "        print(\"üì¶ Engineering product diversity features...\")\n",
    "\n",
    "        product_features = self.df.groupby('CustomerID').agg({\n",
    "            'StockCode': [\n",
    "                ('unique_products', 'nunique'),\n",
    "                ('product_concentration', lambda x: 1 - (x.value_counts().std() / x.value_counts().mean()) if x.value_counts().mean() > 0 else 0)\n",
    "            ],\n",
    "            'Description': [\n",
    "                ('avg_description_length', lambda x: x.str.len().mean())\n",
    "            ]\n",
    "        })\n",
    "        product_features.columns = ['_'.join(col).strip() for col in product_features.columns.values]\n",
    "\n",
    "        self.df['description_words'] = self.df['Description'].str.split().str.len()\n",
    "        category_features = self.df.groupby('CustomerID').agg({\n",
    "            'description_words': [('avg_product_complexity', 'mean')]\n",
    "        })\n",
    "        category_features.columns = ['_'.join(col).strip() for col in category_features.columns.values]\n",
    "\n",
    "        product_features = product_features.join(category_features)\n",
    "        product_features.fillna(0, inplace=True)\n",
    "\n",
    "        self.features['products'] = product_features\n",
    "        print(f\"    ‚úÖ Created {product_features.shape[1]} product diversity features\")\n",
    "        return product_features\n",
    "\n",
    "    def create_behavioral_sequences(self):\n",
    "        \"\"\"Advanced sequential pattern mining\"\"\"\n",
    "        print(\"üîÑ Engineering behavioral sequence features...\")\n",
    "\n",
    "        df_sorted = self.df.sort_values(['CustomerID', 'InvoiceDate'])\n",
    "        sequences = df_sorted.groupby('CustomerID').agg({\n",
    "            'Sales': [\n",
    "                ('purchase_trend', lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0),\n",
    "                ('purchase_acceleration', lambda x: np.polyfit(range(len(x)), x, 2)[0] if len(x) > 2 else 0),\n",
    "                ('purchase_volatility_trend', lambda x: x.rolling(window=min(3, len(x))).std().mean())\n",
    "            ],\n",
    "            'Quantity': [\n",
    "                ('quantity_trend', lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0)\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        sequences.columns = ['_'.join(col).strip() for col in sequences.columns.values]\n",
    "        sequences.fillna(0, inplace=True)\n",
    "\n",
    "        self.features['sequences'] = sequences\n",
    "        print(f\"    ‚úÖ Created {sequences.shape[1]} behavioral sequence features\")\n",
    "        return sequences\n",
    "\n",
    "class EnhancedNLPAnalyzer:\n",
    "    \"\"\"Advanced NLP analysis with multiple embedding strategies\"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.models = {}\n",
    "        \n",
    "    def create_multi_embedding_features(self):\n",
    "        \"\"\"Multiple embedding strategies for product analysis\"\"\"\n",
    "        print(\"üß† Creating advanced NLP embeddings...\")\n",
    "        \n",
    "        try:\n",
    "            # Use a simpler approach without sentence transformers for now\n",
    "            # This can be enhanced when sentence-transformers is available\n",
    "            print(\"    üìù Using basic text features (Sentence Transformers not available)\")\n",
    "            \n",
    "            unique_products = self.df[['StockCode', 'Description']].drop_duplicates()\n",
    "            unique_products['Description_clean'] = unique_products['Description'].fillna('unknown')\n",
    "            \n",
    "            # Simple text features\n",
    "            unique_products['desc_length'] = unique_products['Description_clean'].str.len()\n",
    "            unique_products['desc_words'] = unique_products['Description_clean'].str.split().str.len()\n",
    "            \n",
    "            # Map back to original data\n",
    "            self.df = self.df.merge(\n",
    "                unique_products[['StockCode', 'desc_length', 'desc_words']], \n",
    "                on='StockCode', \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Customer-level aggregation of text features\n",
    "            customer_text_features = self.df.groupby('CustomerID').agg({\n",
    "                'desc_length': 'mean',\n",
    "                'desc_words': 'mean'\n",
    "            }).add_prefix('text_')\n",
    "            \n",
    "            return {'text': customer_text_features}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è NLP features skipped: {e}\")\n",
    "            return {'text': pd.DataFrame()}\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: ADVANCED CLUSTERING FRAMEWORK\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedClusteringFramework:\n",
    "    \"\"\"Comprehensive clustering with ensemble methods and validation\"\"\"\n",
    "    def __init__(self, feature_matrix):\n",
    "        self.X = feature_matrix\n",
    "        self.results = {}\n",
    "        self.ensemble_labels = None\n",
    "\n",
    "    def optimize_clustering_parameters(self):\n",
    "        \"\"\"Hyperparameter optimization for clustering algorithms\"\"\"\n",
    "        print(\"üéØ Optimizing clustering parameters...\")\n",
    "        param_results = {}\n",
    "        if self.X.shape[0] < 2:\n",
    "            print(\"    ‚ùå Not enough samples for clustering.\")\n",
    "            return param_results\n",
    "\n",
    "        # K-Means (more stable than HDBSCAN for small datasets)\n",
    "        kmeans_scores = []\n",
    "        for k in range(2, min(10, self.X.shape[0]//2)):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(self.X)\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                score = silhouette_score(self.X, labels)\n",
    "                kmeans_scores.append((k, score, labels, kmeans))\n",
    "        \n",
    "        if kmeans_scores:\n",
    "            best_kmeans = max(kmeans_scores, key=lambda x: x[1])\n",
    "            param_results['kmeans'] = {\n",
    "                'best_params': {'n_clusters': best_kmeans[0]}, \n",
    "                'score': best_kmeans[1], \n",
    "                'labels': best_kmeans[2],\n",
    "                'model': best_kmeans[3]\n",
    "            }\n",
    "\n",
    "        # HDBSCAN (if available and suitable)\n",
    "        try:\n",
    "            hdbscan_scores = []\n",
    "            for min_size in [max(5, self.X.shape[0]//20), max(10, self.X.shape[0]//15), max(15, self.X.shape[0]//10)]:\n",
    "                if min_size >= self.X.shape[0]: continue\n",
    "                clusterer = hdbscan.HDBSCAN(min_cluster_size=min_size, min_samples=3, metric='euclidean')\n",
    "                labels = clusterer.fit_predict(self.X)\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    score = silhouette_score(self.X, labels)\n",
    "                    hdbscan_scores.append((min_size, score, labels))\n",
    "            \n",
    "            if hdbscan_scores:\n",
    "                best_hdbscan = max(hdbscan_scores, key=lambda x: x[1])\n",
    "                param_results['hdbscan'] = {\n",
    "                    'best_params': {'min_cluster_size': best_hdbscan[0]}, \n",
    "                    'score': best_hdbscan[1], \n",
    "                    'labels': best_hdbscan[2]\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è HDBSCAN skipped: {e}\")\n",
    "\n",
    "        # GMM\n",
    "        try:\n",
    "            gmm_scores = []\n",
    "            for n in range(2, min(8, self.X.shape[0]//3)):\n",
    "                gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "                labels = gmm.fit_predict(self.X)\n",
    "                score = silhouette_score(self.X, labels)\n",
    "                gmm_scores.append((n, score, labels, gmm))\n",
    "            \n",
    "            if gmm_scores:\n",
    "                best_gmm = max(gmm_scores, key=lambda x: x[1])\n",
    "                param_results['gmm'] = {\n",
    "                    'best_params': {'n_components': best_gmm[0]}, \n",
    "                    'score': best_gmm[1], \n",
    "                    'labels': best_gmm[2], \n",
    "                    'model': best_gmm[3]\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è GMM skipped: {e}\")\n",
    "\n",
    "        self.results = param_results\n",
    "        print(f\"    ‚úÖ Best K-Means score: {param_results.get('kmeans', {}).get('score', 0):.3f}\")\n",
    "        print(f\"    ‚úÖ Best HDBSCAN score: {param_results.get('hdbscan', {}).get('score', 0):.3f}\")\n",
    "        print(f\"    ‚úÖ Best GMM score: {param_results.get('gmm', {}).get('score', 0):.3f}\")\n",
    "        return param_results\n",
    "\n",
    "    def ensemble_clustering(self):\n",
    "        \"\"\"Ensemble clustering with consensus mechanism\"\"\"\n",
    "        print(\"ü§ù Creating ensemble clustering...\")\n",
    "        if not self.results:\n",
    "            print(\"    ‚ùå No clustering results to ensemble.\")\n",
    "            return np.zeros(self.X.shape[0])\n",
    "\n",
    "        # Use the best performing algorithm\n",
    "        best_algorithm = None\n",
    "        best_score = -1\n",
    "        for alg_name, result in self.results.items():\n",
    "            if result['score'] > best_score:\n",
    "                best_score = result['score']\n",
    "                best_algorithm = alg_name\n",
    "\n",
    "        if best_algorithm:\n",
    "            self.ensemble_labels = self.results[best_algorithm]['labels']\n",
    "            print(f\"    ‚úÖ Using {best_algorithm} with {len(np.unique(self.ensemble_labels))} clusters\")\n",
    "        else:\n",
    "            self.ensemble_labels = np.zeros(self.X.shape[0])\n",
    "            print(\"    ‚ùå No valid clustering results.\")\n",
    "        \n",
    "        return self.ensemble_labels\n",
    "\n",
    "    def validate_clustering_quality(self, labels):\n",
    "        \"\"\"Comprehensive clustering validation\"\"\"\n",
    "        print(\"üìä Validating clustering quality...\")\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) < 2:\n",
    "            print(\"    ‚ùå Less than 2 clusters found for validation.\")\n",
    "            return {'error': 'Less than 2 clusters found'}\n",
    "        \n",
    "        if self.X.shape[0] != len(labels):\n",
    "            print(\"    ‚ùå Mismatch between feature matrix and labels.\")\n",
    "            return {'error': 'Feature matrix and labels mismatch'}\n",
    "\n",
    "        try:\n",
    "            silhouette = silhouette_score(self.X, labels)\n",
    "            calinski = calinski_harabasz_score(self.X, labels)\n",
    "            davies_bouldin = davies_bouldin_score(self.X, labels)\n",
    "            \n",
    "            metrics = {\n",
    "                'silhouette_score': silhouette,\n",
    "                'calinski_harabasz_score': calinski,\n",
    "                'davies_bouldin_score': davies_bouldin,\n",
    "                'n_clusters': len(unique_labels) - (1 if -1 in labels else 0),\n",
    "                'n_outliers': np.sum(labels == -1) if -1 in labels else 0,\n",
    "            }\n",
    "            \n",
    "            print(f\"    ‚úÖ Silhouette Score: {metrics['silhouette_score']:.3f}\")\n",
    "            print(f\"    ‚úÖ Calinski-Harabasz Score: {metrics['calinski_harabasz_score']:.3f}\")\n",
    "            print(f\"    ‚úÖ Davies-Bouldin Score: {metrics['davies_bouldin_score']:.3f}\")\n",
    "            print(f\"    ‚úÖ Number of Clusters: {metrics['n_clusters']}\")\n",
    "            print(f\"    ‚úÖ Number of Outliers: {metrics['n_outliers']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Validation error: {e}\")\n",
    "            metrics = {'error': str(e)}\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: STATISTICAL ANALYSIS & BUSINESS INSIGHTS\n",
    "# ==============================================================================\n",
    "\n",
    "class BusinessIntelligenceAnalyzer:\n",
    "    \"\"\"Advanced statistical analysis and business insights\"\"\"\n",
    "    def __init__(self, feature_matrix, cluster_labels, original_features):\n",
    "        self.X = feature_matrix\n",
    "        self.labels = cluster_labels\n",
    "        self.features = original_features\n",
    "        self.insights = {}\n",
    "\n",
    "    def statistical_cluster_analysis(self):\n",
    "        \"\"\"Comprehensive statistical analysis of clusters\"\"\"\n",
    "        print(\"üìà Performing statistical cluster analysis...\")\n",
    "        if self.features.empty or self.labels is None or len(self.features) != len(self.labels):\n",
    "            print(\"    ‚ùå Skipping analysis: Data missing or mismatched.\")\n",
    "            return {}\n",
    "\n",
    "        analysis_df = self.features.copy()\n",
    "        analysis_df['cluster'] = self.labels\n",
    "        clustered_df = analysis_df[analysis_df['cluster'] != -1]\n",
    "        if clustered_df.empty:\n",
    "            print(\"    ‚ùå Skipping analysis: No clustered samples found.\")\n",
    "            return {}\n",
    "\n",
    "        cluster_stats = {}\n",
    "        for cluster_id, cluster_data in clustered_df.groupby('cluster'):\n",
    "            stats = {\n",
    "                'size': len(cluster_data),\n",
    "                'percentage': len(cluster_data) / len(clustered_df) * 100\n",
    "            }\n",
    "            cluster_stats[cluster_id] = stats\n",
    "            \n",
    "        self.insights['cluster_statistics'] = cluster_stats\n",
    "        print(f\"    ‚úÖ Analyzed {len(cluster_stats)} clusters\")\n",
    "        return cluster_stats\n",
    "\n",
    "    def customer_lifetime_value_analysis(self):\n",
    "        \"\"\"Advanced CLV analysis by segment\"\"\"\n",
    "        print(\"üí∞ Analyzing Customer Lifetime Value by segment...\")\n",
    "        if 'cluster_statistics' not in self.insights or not self.insights['cluster_statistics']:\n",
    "            print(\"    ‚ùå Skipping CLV analysis: Cluster statistics not available.\")\n",
    "            return {}\n",
    "\n",
    "        analysis_df = self.features.copy()\n",
    "        analysis_df['cluster'] = self.labels\n",
    "        clustered_df = analysis_df[analysis_df['cluster'] != -1]\n",
    "\n",
    "        clv_analysis = {}\n",
    "        for cluster_id, stats in self.insights['cluster_statistics'].items():\n",
    "            cluster_data = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "            \n",
    "            # Safely get values with fallbacks\n",
    "            avg_order_value = cluster_data.get('Sales_avg_order_value', pd.Series([0])).mean()\n",
    "            purchase_frequency = cluster_data.get('InvoiceDate_purchase_frequency', pd.Series([0])).mean()\n",
    "            customer_lifetime = cluster_data.get('customer_lifetime_days', pd.Series([0])).mean()\n",
    "            total_revenue = cluster_data.get('Sales_total_revenue', pd.Series([0])).mean()\n",
    "            \n",
    "            # Calculate CLV with safety checks\n",
    "            if customer_lifetime > 0 and purchase_frequency > 0:\n",
    "                clv = (avg_order_value * purchase_frequency / 365) * customer_lifetime\n",
    "            else:\n",
    "                clv = total_revenue  # Fallback to total revenue\n",
    "\n",
    "            clv_analysis[cluster_id] = {\n",
    "                'predicted_clv': clv,\n",
    "                'avg_order_value': avg_order_value,\n",
    "                'purchase_frequency': purchase_frequency,\n",
    "                'customer_lifetime_days': customer_lifetime,\n",
    "                'total_revenue': total_revenue,\n",
    "                'size': stats['size']\n",
    "            }\n",
    "\n",
    "        self.insights['clv_analysis'] = clv_analysis\n",
    "        print(\"    ‚úÖ CLV analysis completed\")\n",
    "        return clv_analysis\n",
    "\n",
    "    def generate_business_recommendations(self):\n",
    "        \"\"\"Generate actionable business recommendations\"\"\"\n",
    "        print(\"üí° Generating business recommendations...\")\n",
    "        if 'clv_analysis' not in self.insights or not self.insights['clv_analysis']:\n",
    "            print(\"    ‚ùå Skipping recommendations: CLV analysis not available.\")\n",
    "            return {}\n",
    "\n",
    "        recommendations = {}\n",
    "        clv_sorted = sorted(\n",
    "            self.insights['clv_analysis'].items(), \n",
    "            key=lambda x: x[1]['predicted_clv'], \n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        for i, (cluster_id, data) in enumerate(clv_sorted):\n",
    "            if i == 0:  # Highest CLV\n",
    "                segment_type = \"Premium Champions\"\n",
    "                strategy = [\n",
    "                    \"Implement VIP program with exclusive benefits\",\n",
    "                    \"Personalized high-value recommendations\",\n",
    "                    \"Priority customer service\"\n",
    "                ]\n",
    "            elif i == len(clv_sorted) - 1:  # Lowest CLV\n",
    "                segment_type = \"At-Risk/Low Value\"\n",
    "                strategy = [\n",
    "                    \"Re-engagement campaigns with special offers\",\n",
    "                    \"Win-back email sequences\",\n",
    "                    \"Customer feedback surveys\"\n",
    "                ]\n",
    "            else:  # Middle segments\n",
    "                if data['purchase_frequency'] > data['avg_order_value']:\n",
    "                    segment_type = f\"Frequent Buyers (Segment {cluster_id})\"\n",
    "                    strategy = [\n",
    "                        \"Upselling campaigns for higher-value items\",\n",
    "                        \"Bundle deals to increase order value\",\n",
    "                        \"Loyalty program enrollment\"\n",
    "                    ]\n",
    "                else:\n",
    "                    segment_type = f\"High-Value Infrequent (Segment {cluster_id})\"\n",
    "                    strategy = [\n",
    "                        \"Targeted marketing to increase purchase frequency\",\n",
    "                        \"Product education and recommendations\",\n",
    "                        \"Seasonal campaign targeting\"\n",
    "                    ]\n",
    "\n",
    "            recommendations[cluster_id] = {\n",
    "                'segment_type': segment_type,\n",
    "                'strategy': strategy,\n",
    "                'priority': len(clv_sorted) - i,\n",
    "                'metrics': data\n",
    "            }\n",
    "            \n",
    "        self.insights['recommendations'] = recommendations\n",
    "        print(\"    ‚úÖ Business recommendations generated\")\n",
    "        return recommendations\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 5: ADVANCED VISUALIZATION SUITE - COLAB OPTIMIZED\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedVisualizationSuite:\n",
    "    \"\"\"Comprehensive visualization for business stakeholders - Colab optimized\"\"\"\n",
    "\n",
    "    def __init__(self, features_df, cluster_labels, insights):\n",
    "        self.features_df = features_df\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.insights = insights\n",
    "\n",
    "    def create_executive_dashboard(self):\n",
    "        \"\"\"Create comprehensive executive dashboard\"\"\"\n",
    "        print(\"üìä Creating executive dashboard...\")\n",
    "\n",
    "        if self.features_df.empty or self.cluster_labels is None or len(self.features_df) != len(self.cluster_labels):\n",
    "            print(\"    ‚ùå Skipping dashboard: Data missing or mismatched.\")\n",
    "            return\n",
    "\n",
    "        viz_df = self.features_df.copy()\n",
    "        viz_df['cluster'] = self.cluster_labels\n",
    "        viz_df = viz_df[viz_df['cluster'] != -1]\n",
    "        if viz_df.empty:\n",
    "            print(\"    ‚ùå Skipping dashboard: No clustered samples found.\")\n",
    "            return\n",
    "\n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'Customer Segments by Revenue & Size',\n",
    "                'CLV Distribution by Segment',\n",
    "                'Purchase Behavior Patterns',\n",
    "                'Segment Performance Matrix'\n",
    "            ],\n",
    "            specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "                   [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "\n",
    "        # Plot 1: Bubble chart - Revenue vs CLV\n",
    "        if 'clv_analysis' in self.insights and self.insights['clv_analysis']:\n",
    "            for cluster_id, data in self.insights['clv_analysis'].items():\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[data['avg_order_value']], \n",
    "                    y=[data['predicted_clv']], \n",
    "                    mode='markers+text',\n",
    "                    marker=dict(\n",
    "                        size=max(10, np.sqrt(data['size']) * 3),\n",
    "                        opacity=0.7,\n",
    "                        color=cluster_id,\n",
    "                        colorscale='viridis'\n",
    "                    ),\n",
    "                    text=[f\"S{cluster_id}\"], \n",
    "                    textposition='middle center',\n",
    "                    name=f\"Segment {cluster_id}\",\n",
    "                    hovertemplate=f\"<b>Segment {cluster_id}</b><br>\" +\n",
    "                                f\"Avg Order Value: ${data['avg_order_value']:.2f}<br>\" +\n",
    "                                f\"Predicted CLV: ${data['predicted_clv']:.2f}<br>\" +\n",
    "                                f\"Size: {data['size']} customers<extra></extra>\"\n",
    "                ), row=1, col=1)\n",
    "\n",
    "        # Plot 2: CLV bar chart\n",
    "        if 'clv_analysis' in self.insights and self.insights['clv_analysis']:\n",
    "            clv_df = pd.DataFrame(self.insights['clv_analysis']).T.sort_values('predicted_clv', ascending=False)\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=[f\"Segment {idx}\" for idx in clv_df.index], \n",
    "                y=clv_df['predicted_clv'],\n",
    "                name='Predicted CLV',\n",
    "                marker_color='lightblue',\n",
    "                text=[f\"${val:.0f}\" for val in clv_df['predicted_clv']],\n",
    "                textposition='outside'\n",
    "            ), row=1, col=2)\n",
    "\n",
    "        # Plot 3: Purchase patterns\n",
    "        revenue_col = 'Sales_total_revenue'\n",
    "        frequency_col = 'InvoiceDate_purchase_frequency'\n",
    "        \n",
    "        if revenue_col in viz_df.columns and frequency_col in viz_df.columns:\n",
    "            colors = px.colors.qualitative.Set3\n",
    "            for i, cluster_id in enumerate(sorted(viz_df['cluster'].unique())):\n",
    "                cluster_data = viz_df[viz_df['cluster'] == cluster_id]\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=cluster_data[frequency_col], \n",
    "                    y=cluster_data[revenue_col],\n",
    "                    mode='markers', \n",
    "                    name=f\"Segment {cluster_id}\",\n",
    "                    marker=dict(\n",
    "                        color=colors[i % len(colors)],\n",
    "                        opacity=0.6,\n",
    "                        size=8\n",
    "                    ),\n",
    "                    hovertemplate=f\"<b>Segment {cluster_id}</b><br>\" +\n",
    "                                f\"Purchase Frequency: %{{x}}<br>\" +\n",
    "                                f\"Total Revenue: $%{{y:.2f}}<extra></extra>\"\n",
    "                ), row=2, col=1)\n",
    "\n",
    "        # Plot 4: Segment Performance Matrix\n",
    "        if revenue_col in viz_df.columns:\n",
    "            recency_col = 'InvoiceDate_recency_days'\n",
    "            if recency_col not in viz_df.columns:\n",
    "                # Use a fallback column\n",
    "                available_cols = [col for col in viz_df.columns if 'recency' in col.lower()]\n",
    "                if available_cols:\n",
    "                    recency_col = available_cols[0]\n",
    "                else:\n",
    "                    recency_col = viz_df.select_dtypes(include=[np.number]).columns[0]\n",
    "            \n",
    "            cluster_agg = viz_df.groupby('cluster').agg({\n",
    "                revenue_col: 'mean',\n",
    "                recency_col: 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Add size information\n",
    "            cluster_sizes = viz_df.groupby('cluster').size().reset_index(name='size')\n",
    "            cluster_agg = cluster_agg.merge(cluster_sizes, on='cluster')\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=cluster_agg[recency_col], \n",
    "                y=cluster_agg[revenue_col],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    size=cluster_agg['size'] / 2,\n",
    "                    opacity=0.7,\n",
    "                    color=cluster_agg['cluster'],\n",
    "                    colorscale='plasma',\n",
    "                    showscale=True\n",
    "                ),\n",
    "                text=[f'S{c}' for c in cluster_agg['cluster']],\n",
    "                textposition='middle center',\n",
    "                name='Segments',\n",
    "                hovertemplate=\"<b>Segment %{text}</b><br>\" +\n",
    "                            f\"Avg {recency_col}: %{{x:.1f}}<br>\" +\n",
    "                            f\"Avg {revenue_col}: $%{{y:.2f}}<br>\" +\n",
    "                            \"Size: %{marker.size} customers<extra></extra>\"\n",
    "            ), row=2, col=2)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': \"<b>üéØ Customer Segmentation Executive Dashboard</b>\",\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Update axis labels\n",
    "        fig.update_xaxes(title_text=\"Average Order Value ($)\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Predicted CLV ($)\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Predicted CLV ($)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Purchase Frequency\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Total Revenue ($)\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Recency Metric\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Average Revenue ($)\", row=2, col=2)\n",
    "        \n",
    "        # CRITICAL: Use display() to show in Colab\n",
    "        display(fig)\n",
    "        print(\"    ‚úÖ Executive dashboard created and displayed\")\n",
    "\n",
    "    def create_3d_cluster_visualization(self, X_reduced):\n",
    "        \"\"\"Advanced 3D cluster visualization\"\"\"\n",
    "        print(\"üé® Creating 3D cluster visualization...\")\n",
    "        \n",
    "        if X_reduced is None or X_reduced.shape[1] < 2 or self.cluster_labels is None:\n",
    "            print(\"    ‚ùå Skipping 3D visualization: Insufficient data or dimensions.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Use UMAP for 3D embedding\n",
    "            if X_reduced.shape[1] >= 3:\n",
    "                embedding_3d = X_reduced[:, :3]  # Use first 3 dimensions\n",
    "            else:\n",
    "                # Create 3D embedding from 2D\n",
    "                reducer_3d = umap.UMAP(n_components=3, random_state=42, n_neighbors=15)\n",
    "                embedding_3d = reducer_3d.fit_transform(X_reduced)\n",
    "            \n",
    "            # Create color mapping for clusters\n",
    "            unique_labels = np.unique(self.cluster_labels)\n",
    "            colors = px.colors.qualitative.Set3\n",
    "            color_map = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n",
    "            \n",
    "            # Create 3D scatter plot\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            for label in unique_labels:\n",
    "                if label == -1:  # Outliers\n",
    "                    mask = self.cluster_labels == label\n",
    "                    fig.add_trace(go.Scatter3d(\n",
    "                        x=embedding_3d[mask, 0],\n",
    "                        y=embedding_3d[mask, 1],\n",
    "                        z=embedding_3d[mask, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=4,\n",
    "                            color='lightgray',\n",
    "                            opacity=0.6\n",
    "                        ),\n",
    "                        name='Outliers',\n",
    "                        hovertemplate=\"<b>Outlier</b><br>\" +\n",
    "                                    \"X: %{x:.2f}<br>\" +\n",
    "                                    \"Y: %{y:.2f}<br>\" +\n",
    "                                    \"Z: %{z:.2f}<extra></extra>\"\n",
    "                    ))\n",
    "                else:  # Regular clusters\n",
    "                    mask = self.cluster_labels == label\n",
    "                    fig.add_trace(go.Scatter3d(\n",
    "                        x=embedding_3d[mask, 0],\n",
    "                        y=embedding_3d[mask, 1],\n",
    "                        z=embedding_3d[mask, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=5,\n",
    "                            color=color_map[label],\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        name=f'Segment {label}',\n",
    "                        hovertemplate=f\"<b>Segment {label}</b><br>\" +\n",
    "                                    \"X: %{x:.2f}<br>\" +\n",
    "                                    \"Y: %{y:.2f}<br>\" +\n",
    "                                    \"Z: %{z:.2f}<extra></extra>\"\n",
    "                    ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': \"<b>üé® 3D Customer Segmentation Visualization</b>\",\n",
    "                    'x': 0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'font': {'size': 18}\n",
    "                },\n",
    "                scene=dict(\n",
    "                    xaxis_title='Dimension 1',\n",
    "                    yaxis_title='Dimension 2',\n",
    "                    zaxis_title='Dimension 3',\n",
    "                    bgcolor='white'\n",
    "                ),\n",
    "                template='plotly_white',\n",
    "                height=700\n",
    "            )\n",
    "            \n",
    "            # CRITICAL: Use display() to show in Colab\n",
    "            display(fig)\n",
    "            print(\"    ‚úÖ 3D visualization created and displayed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è 3D visualization error: {e}\")\n",
    "            self.create_2d_fallback_visualization(X_reduced)\n",
    "\n",
    "    def create_2d_fallback_visualization(self, X_reduced):\n",
    "        \"\"\"Fallback 2D visualization\"\"\"\n",
    "        print(\"    üìä Creating 2D fallback visualization...\")\n",
    "        \n",
    "        try:\n",
    "            # Use UMAP for 2D embedding\n",
    "            if X_reduced.shape[1] > 2:\n",
    "                reducer_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15)\n",
    "                embedding_2d = reducer_2d.fit_transform(X_reduced)\n",
    "            else:\n",
    "                embedding_2d = X_reduced\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            unique_labels = np.unique(self.cluster_labels)\n",
    "            colors = px.colors.qualitative.Set3\n",
    "            \n",
    "            for i, label in enumerate(unique_labels):\n",
    "                mask = self.cluster_labels == label\n",
    "                if label == -1:\n",
    "                    name = 'Outliers'\n",
    "                    color = 'lightgray'\n",
    "                else:\n",
    "                    name = f'Segment {label}'\n",
    "                    color = colors[i % len(colors)]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=embedding_2d[mask, 0],\n",
    "                    y=embedding_2d[mask, 1],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=8,\n",
    "                        color=color,\n",
    "                        opacity=0.7\n",
    "                    ),\n",
    "                    name=name,\n",
    "                    hovertemplate=f\"<b>{name}</b><br>\" +\n",
    "                                \"X: %{x:.2f}<br>\" +\n",
    "                                \"Y: %{y:.2f}<extra></extra>\"\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=\"üé® 2D Customer Segmentation Visualization\",\n",
    "                xaxis_title='Dimension 1',\n",
    "                yaxis_title='Dimension 2',\n",
    "                template='plotly_white',\n",
    "                height=600\n",
    "            )\n",
    "            \n",
    "            display(fig)\n",
    "            print(\"    ‚úÖ 2D visualization created and displayed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå 2D visualization failed: {e}\")\n",
    "\n",
    "    def create_summary_charts(self):\n",
    "        \"\"\"Create additional summary charts\"\"\"\n",
    "        print(\"üìà Creating summary charts...\")\n",
    "        \n",
    "        if 'clv_analysis' not in self.insights or not self.insights['clv_analysis']:\n",
    "            print(\"    ‚ùå No CLV data for summary charts\")\n",
    "            return\n",
    "        \n",
    "        # Segment size pie chart\n",
    "        fig1 = go.Figure(data=[go.Pie(\n",
    "            labels=[f\"Segment {cid}\" for cid in self.insights['clv_analysis'].keys()],\n",
    "            values=[data['size'] for data in self.insights['clv_analysis'].values()],\n",
    "            hole=.3\n",
    "        )])\n",
    "        \n",
    "        fig1.update_layout(\n",
    "            title=\"ü•ß Customer Distribution by Segment\",\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        display(fig1)\n",
    "        \n",
    "        # CLV comparison\n",
    "        clv_data = self.insights['clv_analysis']\n",
    "        segments = list(clv_data.keys())\n",
    "        clv_values = [clv_data[seg]['predicted_clv'] for seg in segments]\n",
    "        sizes = [clv_data[seg]['size'] for seg in segments]\n",
    "        \n",
    "        fig2 = go.Figure()\n",
    "        fig2.add_trace(go.Bar(\n",
    "            x=[f\"Segment {seg}\" for seg in segments],\n",
    "            y=clv_values,\n",
    "            name='CLV',\n",
    "            yaxis='y',\n",
    "            marker_color='lightblue'\n",
    "        ))\n",
    "        \n",
    "        fig2.add_trace(go.Scatter(\n",
    "            x=[f\"Segment {seg}\" for seg in segments],\n",
    "            y=sizes,\n",
    "            mode='lines+markers',\n",
    "            name='Customer Count',\n",
    "            yaxis='y2',\n",
    "            line=dict(color='orange', width=3)\n",
    "        ))\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            title='üìä CLV vs Customer Count by Segment',\n",
    "            xaxis_title='Segments',\n",
    "            yaxis=dict(title='Predicted CLV ($)', side='left'),\n",
    "            yaxis2=dict(title='Customer Count', side='right', overlaying='y'),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        display(fig2)\n",
    "        print(\"    ‚úÖ Summary charts created\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 6: PREDICTIVE ANALYTICS MODULE\n",
    "# ==============================================================================\n",
    "\n",
    "class PredictiveAnalyticsEngine:\n",
    "    \"\"\"Advanced predictive modeling for customer behavior\"\"\"\n",
    "\n",
    "    def __init__(self, features_df, cluster_labels):\n",
    "        self.features_df = features_df\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "\n",
    "    def build_churn_prediction_model(self):\n",
    "        \"\"\"Advanced churn prediction using ensemble methods\"\"\"\n",
    "        print(\"üîÆ Building churn prediction model...\")\n",
    "        if self.features_df.empty or self.cluster_labels is None: \n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            df = self.features_df.copy()\n",
    "            df['cluster'] = self.cluster_labels\n",
    "            \n",
    "            # Find recency column\n",
    "            recency_cols = [col for col in df.columns if 'recency' in col.lower()]\n",
    "            if not recency_cols:\n",
    "                print(\"    ‚ö†Ô∏è No recency column found for churn prediction\")\n",
    "                return None\n",
    "                \n",
    "            recency_col = recency_cols[0]\n",
    "            \n",
    "            if df[recency_col].nunique() < 2: \n",
    "                return None\n",
    "\n",
    "            churn_threshold = df[recency_col].quantile(0.75)  # Top 25% are considered at risk\n",
    "            df['is_churn'] = (df[recency_col] > churn_threshold).astype(int)\n",
    "\n",
    "            # Prepare features\n",
    "            feature_cols = [col for col in df.columns if col not in ['cluster', 'is_churn']]\n",
    "            X = df[feature_cols].fillna(df[feature_cols].median())\n",
    "            y = df['is_churn']\n",
    "            \n",
    "            if X.empty or len(y.unique()) < 2: \n",
    "                return None\n",
    "\n",
    "            # Use LightGBM for robustness\n",
    "            model = lgb.LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)\n",
    "            \n",
    "            try:\n",
    "                score = np.mean(cross_val_score(model, X, y, cv=min(5, len(y)), scoring='roc_auc'))\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                self.models['churn_prediction'] = {\n",
    "                    'model': model, \n",
    "                    'features': X.columns.tolist(), \n",
    "                    'score': score,\n",
    "                    'threshold': churn_threshold\n",
    "                }\n",
    "                print(f\"    ‚úÖ Churn model AUC: {score:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Churn model training failed: {e}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Churn prediction setup failed: {e}\")\n",
    "            return None\n",
    "            \n",
    "        return self.models.get('churn_prediction')\n",
    "\n",
    "    def build_clv_prediction_model(self):\n",
    "        \"\"\"Advanced Customer Lifetime Value prediction\"\"\"\n",
    "        print(\"üíé Building CLV prediction model...\")\n",
    "        if self.features_df.empty: \n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Find revenue column\n",
    "            revenue_cols = [col for col in self.features_df.columns if 'revenue' in col.lower() or 'sales' in col.lower()]\n",
    "            if not revenue_cols:\n",
    "                print(\"    ‚ö†Ô∏è No revenue column found for CLV prediction\")\n",
    "                return None\n",
    "                \n",
    "            target_col = revenue_cols[0]\n",
    "            \n",
    "            feature_cols = [col for col in self.features_df.columns if col != target_col]\n",
    "            X = self.features_df[feature_cols].fillna(self.features_df[feature_cols].median())\n",
    "            y = self.features_df[target_col]\n",
    "            \n",
    "            if X.empty or y.nunique() < 2: \n",
    "                return None\n",
    "\n",
    "            model = lgb.LGBMRegressor(random_state=42, verbose=-1, n_estimators=100)\n",
    "            \n",
    "            try:\n",
    "                score = np.mean(cross_val_score(model, X, y, cv=min(5, len(y)), scoring='r2'))\n",
    "                model.fit(X, y)\n",
    "\n",
    "                self.models['clv_prediction'] = {\n",
    "                    'model': model, \n",
    "                    'features': X.columns.tolist(), \n",
    "                    'score': score,\n",
    "                    'target_col': target_col\n",
    "                }\n",
    "                print(f\"    ‚úÖ CLV model R¬≤: {score:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è CLV model training failed: {e}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è CLV prediction setup failed: {e}\")\n",
    "            return None\n",
    "            \n",
    "        return self.models.get('clv_prediction')\n",
    "\n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions for all customers\"\"\"\n",
    "        print(\"üéØ Generating customer predictions...\")\n",
    "        \n",
    "        try:\n",
    "            predictions_df = self.features_df.copy()\n",
    "            predictions_df['cluster'] = self.cluster_labels\n",
    "\n",
    "            if 'churn_prediction' in self.models:\n",
    "                model_info = self.models['churn_prediction']\n",
    "                X_churn = predictions_df[model_info['features']].fillna(predictions_df.median())\n",
    "                predictions_df['churn_probability'] = model_info['model'].predict_proba(X_churn)[:, 1]\n",
    "\n",
    "            if 'clv_prediction' in self.models:\n",
    "                model_info = self.models['clv_prediction']\n",
    "                X_clv = predictions_df[model_info['features']].fillna(predictions_df.median())\n",
    "                predictions_df['predicted_future_clv'] = model_info['model'].predict(X_clv)\n",
    "\n",
    "            self.predictions = predictions_df\n",
    "            print(\"    ‚úÖ Predictions generated\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Prediction generation failed: {e}\")\n",
    "            self.predictions = self.features_df.copy()\n",
    "            self.predictions['cluster'] = self.cluster_labels\n",
    "            \n",
    "        return self.predictions\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 7: MAIN EXECUTION PIPELINE - COLAB OPTIMIZED\n",
    "# ==============================================================================\n",
    "\n",
    "class CustomerSegmentationPipeline:\n",
    "    \"\"\"Main orchestration class for the entire analytics pipeline - Colab optimized\"\"\"\n",
    "\n",
    "    def __init__(self, data_path='int_online_tx.csv'):\n",
    "        self.data_path = data_path\n",
    "        self.results = {}\n",
    "\n",
    "    def execute_pipeline(self):\n",
    "        \"\"\"Execute the complete analytics pipeline\"\"\"\n",
    "        print(\"üöÄ EXECUTING ADVANCED CUSTOMER SEGMENTATION PIPELINE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Step 1: Load Data\n",
    "        loader = DataLoader(self.data_path)\n",
    "        df_raw = loader.load_data()\n",
    "        if df_raw is None: \n",
    "            return None\n",
    "\n",
    "        # Step 2: Preprocess Data\n",
    "        df_clean = self.preprocess_data(df_raw)\n",
    "\n",
    "        # Step 3: Feature Engineering\n",
    "        print(\"\\nüîß FEATURE ENGINEERING PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        feature_engineer = AdvancedFeatureEngineer(df_clean)\n",
    "        temporal_features = feature_engineer.create_temporal_features()\n",
    "        product_features = feature_engineer.create_product_diversity_features()\n",
    "        sequence_features = feature_engineer.create_behavioral_sequences()\n",
    "\n",
    "        # Step 4: NLP Analysis\n",
    "        nlp_analyzer = EnhancedNLPAnalyzer(df_clean)\n",
    "        nlp_embeddings = nlp_analyzer.create_multi_embedding_features()\n",
    "\n",
    "        # Step 5: Combine Features\n",
    "        print(\"\\nüîó FEATURE COMBINATION PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        feature_dfs = [temporal_features, product_features, sequence_features]\n",
    "        \n",
    "        # Add NLP features if available\n",
    "        for emb_type, emb_df in nlp_embeddings.items():\n",
    "            if not emb_df.empty:\n",
    "                feature_dfs.append(emb_df)\n",
    "\n",
    "        all_features = temporal_features\n",
    "        for df in feature_dfs[1:]:\n",
    "            if not df.empty:\n",
    "                all_features = all_features.join(df, how='inner')\n",
    "        \n",
    "        all_features = all_features.fillna(0)\n",
    "\n",
    "        # Remove non-variant features\n",
    "        non_variant_cols = all_features.columns[all_features.nunique() <= 1]\n",
    "        if not non_variant_cols.empty:\n",
    "            all_features.drop(non_variant_cols, axis=1, inplace=True)\n",
    "            print(f\"üóëÔ∏è Removed {len(non_variant_cols)} non-variant features.\")\n",
    "\n",
    "        print(f\"\\nüìä FINAL FEATURE MATRIX: {all_features.shape[0]:,} customers √ó {all_features.shape[1]} features\")\n",
    "\n",
    "        if all_features.empty or all_features.shape[0] < 5:\n",
    "            print(\"    ‚ùå Insufficient data for analysis\")\n",
    "            return None\n",
    "        \n",
    "        # Step 6: Feature Scaling\n",
    "        print(\"\\n‚öñÔ∏è FEATURE SCALING PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        features_scaled = RobustScaler().fit_transform(all_features)\n",
    "        print(\"    ‚úÖ Features scaled using RobustScaler\")\n",
    "\n",
    "        # Step 7: Clustering\n",
    "        print(\"\\nüéØ CLUSTERING PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        clustering_framework = AdvancedClusteringFramework(features_scaled)\n",
    "        clustering_framework.optimize_clustering_parameters()\n",
    "        ensemble_labels = clustering_framework.ensemble_clustering()\n",
    "        cluster_metrics = clustering_framework.validate_clustering_quality(ensemble_labels)\n",
    "\n",
    "        # Step 8: Business Intelligence\n",
    "        print(\"\\nüíº BUSINESS INTELLIGENCE PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        bi_analyzer = BusinessIntelligenceAnalyzer(features_scaled, ensemble_labels, all_features)\n",
    "        cluster_stats = bi_analyzer.statistical_cluster_analysis()\n",
    "        clv_analysis = bi_analyzer.customer_lifetime_value_analysis()\n",
    "        recommendations = bi_analyzer.generate_business_recommendations()\n",
    "\n",
    "        # Step 9: Visualizations\n",
    "        print(\"\\nüìä VISUALIZATION PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        viz_suite = AdvancedVisualizationSuite(all_features, ensemble_labels, bi_analyzer.insights)\n",
    "        viz_suite.create_executive_dashboard()\n",
    "        \n",
    "        if features_scaled.shape[1] >= 2:\n",
    "            viz_suite.create_3d_cluster_visualization(features_scaled)\n",
    "            \n",
    "        viz_suite.create_summary_charts()\n",
    "\n",
    "        # Step 10: Predictive Analytics\n",
    "        print(\"\\nüîÆ PREDICTIVE ANALYTICS PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        predictive_engine = PredictiveAnalyticsEngine(all_features, ensemble_labels)\n",
    "        churn_model = predictive_engine.build_churn_prediction_model()\n",
    "        clv_model = predictive_engine.build_clv_prediction_model()\n",
    "        predictions = predictive_engine.generate_predictions()\n",
    "\n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'raw_data': df_raw,\n",
    "            'clean_data': df_clean,\n",
    "            'feature_matrix': all_features,\n",
    "            'scaled_features': features_scaled,\n",
    "            'cluster_labels': ensemble_labels,\n",
    "            'cluster_metrics': cluster_metrics,\n",
    "            'cluster_statistics': cluster_stats,\n",
    "            'clv_analysis': clv_analysis,\n",
    "            'business_recommendations': recommendations,\n",
    "            'churn_model': churn_model,\n",
    "            'clv_model': clv_model,\n",
    "            'customer_predictions': predictions,\n",
    "            'data_profile': loader.data_profile\n",
    "        }\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def preprocess_data(self, df_raw):\n",
    "        \"\"\"Enhanced data preprocessing\"\"\"\n",
    "        print(\"\\nüßπ DATA PREPROCESSING PHASE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        initial_rows = len(df_raw)\n",
    "        \n",
    "        # Basic cleaning\n",
    "        df = df_raw.dropna(subset=['CustomerID']).copy()\n",
    "        df['CustomerID'] = df['CustomerID'].astype(str)  # More flexible\n",
    "        \n",
    "        # Remove negative quantities and prices\n",
    "        df = df[df['Quantity'] > 0]\n",
    "        df = df[df['UnitPrice'] > 0]\n",
    "        \n",
    "        # Create sales column\n",
    "        df['Sales'] = df['Quantity'] * df['UnitPrice']\n",
    "        \n",
    "        # Remove extreme outliers using IQR method\n",
    "        Q1 = df['Sales'].quantile(0.25)\n",
    "        Q3 = df['Sales'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Keep only data within bounds\n",
    "        df = df[~((df['Sales'] < lower_bound) | (df['Sales'] > upper_bound))]\n",
    "        \n",
    "        final_rows = len(df)\n",
    "        removed_rows = initial_rows - final_rows\n",
    "        \n",
    "        print(f\"    ‚úÖ Cleaned data: {df.shape[0]:,} transactions, {df['CustomerID'].nunique():,} customers\")\n",
    "        print(f\"    üìâ Removed {removed_rows:,} rows ({removed_rows/initial_rows*100:.1f}% of original data)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def generate_executive_summary(self):\n",
    "        \"\"\"Generate comprehensive executive summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã EXECUTIVE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not self.results: \n",
    "            print(\"    ‚ùå No results to summarize.\")\n",
    "            return\n",
    "\n",
    "        # Data Overview\n",
    "        print(f\"\\nüìä DATA OVERVIEW:\")\n",
    "        print(f\"    ‚Ä¢ Total Customers Analyzed: {self.results['feature_matrix'].shape[0]:,}\")\n",
    "        print(f\"    ‚Ä¢ Total Features Created: {self.results['feature_matrix'].shape[1]:,}\")\n",
    "        print(f\"    ‚Ä¢ Data Quality Score: {(1 - self.results['data_profile']['missing_values'].sum() / len(self.results['clean_data'])) * 100:.1f}%\")\n",
    "\n",
    "        # Clustering Results\n",
    "        print(f\"\\nüéØ SEGMENTATION RESULTS:\")\n",
    "        metrics = self.results['cluster_metrics']\n",
    "        if 'error' not in metrics:\n",
    "            print(f\"    ‚Ä¢ Clusters Identified: {metrics.get('n_clusters', 'N/A')}\")\n",
    "            print(f\"    ‚Ä¢ Clustering Quality (Silhouette): {metrics.get('silhouette_score', 0):.3f}\")\n",
    "            print(f\"    ‚Ä¢ Outliers Detected: {metrics.get('n_outliers', 0):,}\")\n",
    "\n",
    "        # Segment Overview\n",
    "        print(f\"\\nüí∞ SEGMENT PERFORMANCE:\")\n",
    "        if 'clv_analysis' in self.results and self.results['clv_analysis']:\n",
    "            clv_data = self.results['clv_analysis']\n",
    "            total_clv = sum(data['predicted_clv'] * data['size'] for data in clv_data.values())\n",
    "            total_customers = sum(data['size'] for data in clv_data.values())\n",
    "            \n",
    "            print(f\"    ‚Ä¢ Average CLV Across All Segments: ${total_clv/total_customers if total_customers > 0 else 0:.2f}\")\n",
    "            \n",
    "            # Sort segments by CLV\n",
    "            sorted_segments = sorted(clv_data.items(), key=lambda x: x[1]['predicted_clv'], reverse=True)\n",
    "            \n",
    "            for i, (seg_id, data) in enumerate(sorted_segments[:3]):  # Top 3 segments\n",
    "                print(f\"    ‚Ä¢ Segment {seg_id}: {data['size']:,} customers, Avg CLV: ${data['predicted_clv']:.2f}\")\n",
    "\n",
    "        # Model Performance\n",
    "        print(f\"\\nü§ñ PREDICTIVE MODEL PERFORMANCE:\")\n",
    "        if 'churn_model' in self.results and self.results['churn_model']:\n",
    "            print(f\"    ‚Ä¢ Churn Prediction AUC: {self.results['churn_model']['score']:.3f}\")\n",
    "        if 'clv_model' in self.results and self.results['clv_model']:\n",
    "            print(f\"    ‚Ä¢ CLV Prediction R¬≤: {self.results['clv_model']['score']:.3f}\")\n",
    "\n",
    "        # Key Recommendations\n",
    "        print(f\"\\nüí° KEY BUSINESS RECOMMENDATIONS:\")\n",
    "        if 'business_recommendations' in self.results and self.results['business_recommendations']:\n",
    "            recs = sorted(\n",
    "                self.results['business_recommendations'].values(), \n",
    "                key=lambda x: x['priority'], \n",
    "                reverse=True\n",
    "            )\n",
    "            for i, rec in enumerate(recs[:3], 1):\n",
    "                print(f\"    {i}. {rec['segment_type']}: {rec['strategy'][0]}\")\n",
    "        \n",
    "        print(\"\\nüéâ PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    def display_sample_predictions(self, n_samples=10):\n",
    "        \"\"\"Display sample customer predictions\"\"\"\n",
    "        if 'customer_predictions' not in self.results or self.results['customer_predictions'].empty:\n",
    "            print(\"    ‚ùå No customer predictions available\")\n",
    "            return\n",
    "            \n",
    "        predictions = self.results['customer_predictions']\n",
    "        sample_predictions = predictions.head(n_samples)\n",
    "        \n",
    "        print(f\"\\nüìã SAMPLE CUSTOMER PREDICTIONS (Top {n_samples}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display columns that exist\n",
    "        display_cols = ['cluster']\n",
    "        if 'churn_probability' in predictions.columns:\n",
    "            display_cols.append('churn_probability')\n",
    "        if 'predicted_future_clv' in predictions.columns:\n",
    "            display_cols.append('predicted_future_clv')\n",
    "            \n",
    "        # Add some key features for context\n",
    "        feature_cols = [col for col in predictions.columns if 'revenue' in col.lower() or 'frequency' in col.lower()][:3]\n",
    "        display_cols.extend(feature_cols)\n",
    "        \n",
    "        print(sample_predictions[display_cols].round(2).to_string())\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    def export_results(self, filename='customer_segmentation_results.csv'):\n",
    "        \"\"\"Export results to CSV\"\"\"\n",
    "        if 'customer_predictions' not in self.results:\n",
    "            print(\"    ‚ùå No results to export\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.results['customer_predictions'].to_csv(filename)\n",
    "            print(f\"    ‚úÖ Results exported to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Export failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 8: EXECUTION - COLAB OPTIMIZED\n",
    "# ==============================================================================\n",
    "\n",
    "def run_customer_segmentation_analysis(data_path='int_online_tx.csv'):\n",
    "    \"\"\"\n",
    "    Main function to run the complete customer segmentation analysis\n",
    "    Optimized for Google Colab\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize and run pipeline\n",
    "        pipeline = CustomerSegmentationPipeline(data_path)\n",
    "        results = pipeline.execute_pipeline()\n",
    "\n",
    "        if results:\n",
    "            print(f\"\\n‚úÖ Analysis completed successfully!\")\n",
    "            print(f\"üìä Results stored in pipeline.results dictionary\")\n",
    "            \n",
    "            # Generate executive summary\n",
    "            pipeline.generate_executive_summary()\n",
    "            \n",
    "            # Display sample predictions\n",
    "            pipeline.display_sample_predictions()\n",
    "            \n",
    "            # Optional: Export results\n",
    "            # pipeline.export_results()\n",
    "            \n",
    "            return pipeline\n",
    "        else:\n",
    "            print(\"‚ùå Analysis failed - no results generated\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# COLAB SETUP INSTRUCTIONS AND EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"Setup function for Google Colab environment\"\"\"\n",
    "    print(\"üîß Setting up Colab environment...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = [\n",
    "        'umap-learn',\n",
    "        'hdbscan', \n",
    "        'sentence-transformers',\n",
    "        'xgboost',\n",
    "        'lightgbm',\n",
    "        'plotly',\n",
    "        'networkx'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"    ‚úÖ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"    üì¶ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "    \n",
    "    print(\"    ‚úÖ Environment setup complete!\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ ADVANCED CUSTOMER SEGMENTATION ANALYTICS SUITE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìã GOOGLE COLAB OPTIMIZED VERSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Uncomment the next line if running for the first time in Colab\n",
    "    # setup_colab_environment()\n",
    "    \n",
    "    # Run the analysis\n",
    "    pipeline = run_customer_segmentation_analysis()\n",
    "    \n",
    "    if pipeline and pipeline.results:\n",
    "        print(\"\\nüéØ QUICK ACCESS TO RESULTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"‚Ä¢ pipeline.results['feature_matrix'] - Customer features\")\n",
    "        print(\"‚Ä¢ pipeline.results['cluster_labels'] - Segment assignments\") \n",
    "        print(\"‚Ä¢ pipeline.results['clv_analysis'] - CLV by segment\")\n",
    "        print(\"‚Ä¢ pipeline.results['business_recommendations'] - Action items\")\n",
    "        print(\"‚Ä¢ pipeline.results['customer_predictions'] - Individual predictions\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display key insights\n",
    "        if 'business_recommendations' in pipeline.results:\n",
    "            print(f\"\\nüí° TOP BUSINESS INSIGHTS:\")\n",
    "            recs = pipeline.results['business_recommendations']\n",
    "            for cluster_id, rec_data in list(recs.items())[:3]:\n",
    "                print(f\"\\nüéØ {rec_data['segment_type']} (Segment {cluster_id}):\")\n",
    "                for strategy in rec_data['strategy'][:2]:\n",
    "                    print(f\"   ‚Ä¢ {strategy}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Analysis Complete! All visualizations should be displayed above.\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
